# Smashing all Graphics APIs. (OpenGL Edition)

As the new year started, I got one goal.

**Ah! Let's make a portfolio for representative graphics APIs in the first half of this year!**

Hmm... so I asked GPT for the difficulty order, and this punk.  
It said OpenGL -> DX11 -> DX12 -> Vulkan.

So, I started with OpenGL. Yeah.

(Note: This post is good for anyone who knows even a little about graphics.)

## 1. Always start with what to do...

While proceeding with the Penitence project, I felt that I was more of a noob than I thought.

So, for this project, I decided to make it without worrying too much about the structure.

There is music I always listen to when winter comes.  
Not someone else's music, but my song...  

[![Video Label](http://img.youtube.com/vi/BgxQtsXQ5bI/0.jpg)](https://youtu.be/BgxQtsXQ5bI?si=tngBCSk-wW72bBQr)

The title is 'White Ashes'.

Late winter of 2021,   

I was having a really hard time mentally. (I still don't know the reason.)  
It's a song I composed for 3 hours based on a melody that came to mind at dawn,

I was a smoker (currently 4 months smoke-free) and to monitor it (I still remember),  
I bit a cigarette on the first floor at 4 AM, plugged in my AirPods, looked at the sky, and

***It snowed.***

Strangely, that scene melted well with the song.  
The falling cigarette ash mixed with the falling snow... anyway, it was like that.  

So the title of this song is 'White **Ashes**'.

Since that day, I always listen to this song when it snows.  
To me, it's more touching than a carol. 

~~Shibal! Jiwon hyung! So what did you do with this~!~~

Ah, I poured out a story from when I was too much of an F (Feeler).  
Anyway, I vowed to give a small gift to this song on this occasion.  

> **Music Video after 3 years**

This was the beginning of this project.

## 2. Concept?

Hmm... actually the first thing that came to mind was the MV of this song.

[![Video Label](http://img.youtube.com/vi/Aq_gsctWHtQ/0.jpg)](https://youtu.be/Aq_gsctWHtQ?si=4ifkV95M_fUcdPwX)

**The Black Skirts - Everything**

My favorite scene in this MV is  
the scene where numerous red chairs fly away, and I wanted to pay homage to this.

At first, I started thinking only of the **feeling**...
of white cubes roughly falling on a black screen.

## 3. Development Start!

Unlike the previous project, Penitence, I tried to avoid planting external libraries inside the project as much as possible.

OpenGL is already included in the latest Mac, so Pass.  
Install glfw, glm with brew... set up CMakeList too... then?  

Now settings are done! Development begins.

## 4. Rendering Pipeline? GLSL?

Let's grasp the concept firmly.  

### Rendering Pipeline means the process in which data is displayed on the display.

It is structured as a kind of pipeline serial structure where each stage references, calculates, processes data, and passes it to the next stage.

So, this pipeline must be run for each pixel.  
But, do you happen to know how many pixels a modern monitor has?  
Based on HD, it consists of **1920 x 1080** pixels!

If it were a CPU, doing this operation,  

**"Aah! Master! It's too hard!! ㅜㅜ"**  

It would say, but the GPU is different.
Since the GPU is hardware specialized for simple parallel operations (like matrix multiplication...),  
It can run the pixel pipeline faster than the CPU.

Hmm... if there is hardware, software must also exist.  
What do we program GPU software with? C++? Java?  

Well, we are using OpenGL.

### GLSL (Graphics Library Shader Language) is a GPU programming language specialized for the OpenGL pipeline.

Since there are many better posts referring to other blogs and writings for OpenGL's pipeline,  
I will omit it. Instead, I will put more weight on the connection between the pipeline and glsl.

The most important things in GLSL are the following two.

- Vertex Shader
- Fragment Shader

### Vertex Shader is a program about geometric data.
Well... information like geometric position or color? You can pass them as attributes in the programming stage.

Below is the actual code I used.

```c
// Vertex GLSL
#version 330 core

layout(location = 0) in vec3 AttPos;
layout(location = 1) in vec3 AttNorm;
layout(location = 2) in vec3 AttColor;

out vec3 InPos;
out vec3 InNorm;
out vec3 InColor;

// MVP
uniform mat4 Model;
uniform mat4 View;
uniform mat4 Proj;

void main(){
  InPos = vec3(Model * vec4(AttPos, 1.0));
  InNorm = normalize(mat3(transpose(inverse(Model))) * AttNorm);
  InColor = AttColor;

  gl_Position = Proj * View * Model * vec4(InPos, 1.0);
}
```

Um... first, we need to think about the in and out fields.

in is literally data input to the shader, out is data going out **somewhere**.  
The processed data based on the data coming in as in, the outs go into the fragment shader again.

Uniforms are just, settings for transformation? It's easy to see it that way.  
Vertex data is stored in gl_Position, and at this time, coordinates are transformed through the MVP matrix.

---

Now, then what does the fragment shader do?

### Fragment Shader plays the role of deciding the actual data shown.

Things related to color are processed here.

I'll see my code here too!

```C
#version 330 core

in vec3 InPos;
in vec3 InNorm;
in vec3 InColor;

out vec4 FragColor;

uniform vec3 LightPos;
uniform vec3 LightColor;
uniform vec3 ViewPos;

uniform bool IsRandomEnabled;

void main() {
  vec3 amb = 0.1 * LightColor;

  vec3 lightDir = normalize(LightPos - InPos);
  float diff = max(0.0, dot(InNorm, lightDir));
  vec3 diffColor = diff * LightColor;

  vec3 viewDir = normalize(ViewPos - InPos);
  vec3 reflectDir = reflect(-lightDir, InNorm);
  float spec = pow(max(dot(viewDir, reflectDir), 0.0), 0.5);
  vec3 specColor = 5.0 * spec * LightColor;

  vec3 extra = vec3(0.0, 0.0, 0.0);

  if(IsRandomEnablednabled) extra = InNorm;

  vec3 result = (amb + diff + spec + extra) * InColor;
  FragColor = vec4(result, 1.0);
}
```

Hmm... it's a bit complex, but I'll tear it apart one by one.  

Summarizing the code above, it's a process of phong color blending for the incoming information. [(Phong Color Explanation)](https://en.wikipedia.org/wiki/Phong_reflection_model)  

On top of that, if you input a specific command (enabled), extra changes.  
**The role of this is to change even the color of the randomly generated cube to random.**

Hmm? What does that mean?

Since randomly generated cubes have different positions and rotations, **almost all of them will be different**.  
And if you map that vector to a color value? It looks like random colors are applied to all of them!

The processed color is stored in frag_color and displayed!

---

Like this, I really just dipped my toes into the shader structure of OpenGL.

Now the next topic?

## 5. Post-processing

My favorite thing in the Everything MV is the retro vibe.  
And actually, this is also the **core** of this project.

This effect is called graphics post-processing.  
If you say this in English, you know what it is, right?  

Now, then how are you going to implement post-processing?  
That is also actually written in glsl.

```C
#version 330 core

layout(location = 0) in vec2 AttPos;
layout(location = 1) in vec2 AttTexCoords;

out vec2 InTexCoords;

void main(){
  InTexCoords = AttTexCoords;
  gl_Position = vec4(AttPos, 0.0, 1.0);
}
```

Actually, the vertex shader doesn't do much role.  
It's a role of moving the vertex input in the program code to the fragment.

```C
#version 330 core

in vec2 InTexCoords;

out vec4 frag_color;

uniform sampler2D ScreenTex;
uniform vec2 ScreenRes;

float rand(vec2 coords) {
  return fract(sin(dot(coords.xy, vec2(12.9898, 78.233))) * 43758.5453);
}

void main() {
  vec3 color = texture(ScreenTex, InTexCoords).rgb;
  float lineThick = 1.0;
  float intensity = 0.2;

  float y = InTexCoords.y * ScreenRes.y;

  if(mod(y, 2.0) < lineThick) color *= (1.0 - intensity);

  float noiseStrength = 0.05;
  float noise = rand(InTexCoords) * noiseStrength;
  color += vec3(noise);
  FragColor = vec4(color, 1.0);
}
```

The code is simple.

1. Scanline - If you look closely at old CRT monitors, there are horizontal lines? haze? kind of lines, implemented that. Depending on the screen coordinate y value, (here even lines) adjusted the intensity of the color to implement a similar effect.

2. Film Grain (Noise) Effect - This is just a method of adjusting random coordinate pixel colors with the famous random sine noise function.

Hmm... actually there's nothing much.  
Really nothing much ㅜㅜ

## 6. Result rather than process, punk

Hmm... I'll show you my MV made like that.

[![Video Label](http://img.youtube.com/vi/xw5fyrhhpnw/0.jpg)](https://youtu.be/xw5fyrhhpnw?si=JMcxg5iPCu7cG-WD)

<div align='center'>_Grammer - White ashes</div>
_(Please leave a lot of comments~)_

No, I think a much more amazing result came out than I thought.  
Excluding cutting front and back, resizing, **it's a one-take video without cut editing.**  

### Behind cuts.

Actually, the device that recorded all of this is a **MacBook (2017)**.  
This is a dope fact, if you watch the video, there is no stuttering!

Clearly, I recorded with OBS too... how did it happen?

**Rendering Optimization????!??**  
No, I just turned off VSCode and ran it with terminal...  
It ran well. heh

## 7. Wrap-up

Starting with this post, the next 3 posts are planned to cover all graphics APIs.  

Please don't expect too much no no haha